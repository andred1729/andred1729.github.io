---
layout: page
title: mobileGenius
description: robot run by an FPGA running a multimodal LLM
img:
importance: 1
category: current
---

    --- In Progress!


<span style="font-size:1.1em; font-weight:bold;">Abstract:</span>

The primary goal of our project is to successfully deploy and run a multimodal Large Language Model (LLM) entirely within a Field Programmable Gate Array (FPGA). Specifically, we aim to integrate computer vision capabilities directly into the FPGA, enabling the model to interpret visual data, make real-time decisions, and interact physically with its environment (such as recognizing and following objects or responding to voice commands).

The key innovation of this approach lies in leveraging the FPGAâ€™s programmable circuitry to drastically enhance inference efficiency and reduce energy consumption compared to conventional AI hardware like GPUs or TPUs. By using an FPGA, we can optimize hardware directly for our model's needs, enabling faster inference speeds and lower power usage, which are both critical factors for edge computing applications.

To our knowledge, a project combining FPGA and multimodal AI at this level of integration has not been attempted before. Previous FPGA-AI work has primarily focused on improving inference speed or efficiency, rather than deploying complete edge-computing solutions that incorporate real-time multimodal AI input-output interactions as we propose with our mobileGenius project.

We believe that our work will be directly transferable to the field of robotics, alongside edge computing applications.
